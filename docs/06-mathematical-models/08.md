**[返回目录](../INDEX.md)** | **[返回本章](./README.md)** | **[上一节](./07.md)** | **[下一节](./99.md)**

# 06.8 参数优化
### 8.1 梯度下降

#### 8.1.1 定义

梯度下降：

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

其中：
- $\theta$：参数向量
- $\alpha$：学习率
- $J(\theta)$：损失函数

#### 8.1.2 学习率选择

学习率 $\alpha$ 的选择：
- $\alpha$ 太大：发散
- $\alpha$ 太小：收敛慢
- 经验约束：\(\alpha \in [\alpha_{\min}, \alpha_{\max}]\)（边界用符号表示）

**参数解释**：
- \(\alpha_{\min}\)：学习率下界（不展示真实数值）
- \(\alpha_{\max}\)：学习率上界（不展示真实数值）

### 8.2 贝叶斯优化

#### 8.2.1 定义

贝叶斯优化适用于黑盒函数优化：

1. 构建代理模型（如高斯过程）
2. 采集函数选择下一个采样点
3. 更新代理模型
4. 重复直到收敛

#### 8.2.2 采集函数

采集函数示例：

**Expected Improvement (EI)**：

$$
EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]
$$

其中：
- $f(x)$：目标函数
- $f(x^+)$：当前最优值

---

**[返回目录](../INDEX.md)** | **[返回本章](./README.md)** | **[上一节](./07.md)** | **[下一节](./99.md)**
